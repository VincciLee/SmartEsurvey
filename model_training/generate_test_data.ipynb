{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Programmer Name: Ms Lee Wen Xi, APD3F2211CS(IS)\n",
    "# Program Name: generate_test_data.ipynb\n",
    "# Description: A script to retrieve 50 data rows randomly from the full dataset\n",
    "# First Written On: 26/05/2023\n",
    "# Last Edited On:  22/07/2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "nltk_stop_words = set(stopwords.words('english'))\n",
    "punc = list(string.punctuation) + list('‘’')\n",
    "stop_words = []\n",
    "train_header = 'clean_text'\n",
    "target = 'category'\n",
    "\n",
    "# Lexicon of negation cues references: Negation Scope Detection for Twitter Sentiment Analysis + manual filter from nltk stop words\n",
    "negations = ['aint', 'doesnt', 'havent', 'lacks', 'none', 'mightnt', 'shouldnt', 'cannot', 'dont', 'havnt', 'neither', 'nor', 'mustnt', 'wasnt', 'cant', 'hadnt', 'isnt', 'never', 'not', 'neednt', 'without', 'darent' 'hardly', 'lack', 'no', 'nothing', 'oughtnt', 'wouldnt', 'didnt', 'hasnt', 'lacking', 'nobody', 'nowhere', 'shant', 'ain', 'doesn', 'haven', 'mightn', 'shouldn', 'havn', 'mustn', 'wasn', 'hadn', 'isn', 'needn', 'oughtn', 'wouldn', 'didn', 'hasn', 'shan', 'couldn', 'won', 'don', 'aren', 'arent', 'weren', 'werent' 'against']\n",
    "rneg = r\"[A-Za-z]{1,}n't\\b\"\n",
    "\n",
    "for w in nltk_stop_words:\n",
    "    if not w in negations and not re.match(rneg, w):\n",
    "        stop_words.append(w)\n",
    "  \n",
    "def remove_emojis(data):\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, '', data)\n",
    "\n",
    "def remove_tags(string):\n",
    "    result = re.sub(r'@[A-Za-z0-9]{0,}(\\s|\\b)', '', string)   #remove @ tags\n",
    "    result = re.sub(r'#[A-Za-z0-9]{0,}(\\s|\\b)', '', result)   #remove # tags\n",
    "    result = re.sub(r'\\b((http|https):\\/\\/)[-a-zA-Z0-9@:%._\\\\+~#?&\\/\\/=]{0,}','',result)   #remove URLs\n",
    "    result = remove_emojis(result)    # remove emojis\n",
    "    result = result.lower()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter_Data records\n",
    "data = pd.read_csv('Datasets\\\\Twitter Sentiment Dataset\\\\Twitter_Data.csv')\n",
    "data = data.dropna()\n",
    "data = data.sample(frac = 1)\n",
    "data = data.head(50)\n",
    "\n",
    "data[train_header] = data[train_header].apply(lambda cw : remove_tags(cw)) \n",
    "data[train_header] = data[train_header].apply(lambda x: x.replace('\"', ''))\n",
    "data[train_header] = data[train_header].apply(lambda x: ' '.join([word for word in word_tokenize(x) if word not in (punc)]))\n",
    "data[train_header] = data[train_header].apply(lambda x: ' '.join([word for word in word_tokenize(x) if word not in (stop_words)]))\n",
    "data[train_header] = data[train_header].apply(lambda x: re.sub('\\W+',' ',x).strip())\n",
    "data[train_header] = data[train_header].str.strip()\n",
    "\n",
    "data.loc[data[target] == 1.0, target] = 'Positive'\n",
    "data.loc[data[target] == 0.0, target] = 'Neutral'\n",
    "data.loc[data[target] == -1.0, target] = 'Negative'\n",
    "\n",
    "data = data.loc[:, [train_header, target]]\n",
    "data.rename(columns = {train_header:'What is your opinion about the election?'}, inplace = True)\n",
    "data.rename(columns = {target:'What is your opinion about the election?_Sentiment'}, inplace = True)\n",
    "\n",
    "data.to_csv('Demo_Test_Data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
